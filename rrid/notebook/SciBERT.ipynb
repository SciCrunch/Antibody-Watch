{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XAQsAt0rZYy9"
   },
   "source": [
    "# Task 2: RRID Linking model with SciBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3joJd3MeZxPK"
   },
   "source": [
    "## 1.1 Using Colab for GPU Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "TjCzqsWhZr39",
    "outputId": "bcc15531-1a49-4ea9-9208-400b90a67a3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zlK5YRq_Z9CB"
   },
   "source": [
    "In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "9cMgnPHgZ5gO",
    "outputId": "d13e7164-e4fc-4a58-9946-80393df40223"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla P4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1zfdZuuiaCZx"
   },
   "source": [
    "## 1.2 Installing the Hugging Face Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "colab_type": "code",
    "id": "fBuuazcgaAlb",
    "outputId": "b8cc7fa9-3264-456d-87f1-46251fbce335"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.11.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EgcAsCIDaR1k"
   },
   "source": [
    "## 2. Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJDLDekcaV3M"
   },
   "source": [
    "### 2.1 Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QXrwoxH-aag3"
   },
   "source": [
    "Mounted Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "IXpGkNLAaJuW",
    "outputId": "bbc6a10b-f05e-4e76-d755-18fce02e477b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kbr2jwR3aiHv"
   },
   "source": [
    "Set path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bAR3WbObacRz"
   },
   "outputs": [],
   "source": [
    "folderPath = '/content/gdrive/My Drive/antibody-watch/antibody-rrid/'\n",
    "dataPath = '/content/gdrive/My Drive/antibody-watch/antibody-rrid/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIMnydb4azJk"
   },
   "source": [
    "### 2.2 Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "IOT6hJkOavK1",
    "outputId": "90d488eb-b689-4859-87fd-4216281d658f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>PMID</th>\n",
       "      <th>RRID Snippet_rdw_x</th>\n",
       "      <th>SNIPPET_3192antibody_x</th>\n",
       "      <th>RRID GT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>23390418</td>\n",
       "      <td>The four Panx1 antibodies used in this study w...</td>\n",
       "      <td>The Diatheva and Dahl antibodies were two of t...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>23390418</td>\n",
       "      <td>(2) chicken anti-Panx1 (obtained from Gerhard ...</td>\n",
       "      <td>The Diatheva and Dahl antibodies were two of t...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>23390418</td>\n",
       "      <td>(3) rabbit anti-Panx1 (denoted as Rb57; NIF an...</td>\n",
       "      <td>The Diatheva and Dahl antibodies were two of t...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>23390418</td>\n",
       "      <td>and (4) mouse anti-Panx1 (denoted as Mo503; NI...</td>\n",
       "      <td>The Diatheva and Dahl antibodies were two of t...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>23390418</td>\n",
       "      <td>The four Panx1 antibodies used in this study w...</td>\n",
       "      <td>For example, in the study of Panx1 knockout mi...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      PMID  ...                             SNIPPET_3192antibody_x RRID GT\n",
       "0   0  23390418  ...  The Diatheva and Dahl antibodies were two of t...     Yes\n",
       "1   1  23390418  ...  The Diatheva and Dahl antibodies were two of t...     Yes\n",
       "2   2  23390418  ...  The Diatheva and Dahl antibodies were two of t...      No\n",
       "3   3  23390418  ...  The Diatheva and Dahl antibodies were two of t...      No\n",
       "4   4  23390418  ...  For example, in the study of Panx1 knockout mi...     Yes\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(dataPath+'all-pair.csv')\n",
    "df = df.loc[df['RRID GT'] != '-' ]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wK6NWhdEa9sY"
   },
   "source": [
    "### 2.3 Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DYLeK0LZbLog"
   },
   "source": [
    "The two properties we actually care about are the the `sentence` and its `label`, which is referred to as the \"acceptibility judgment\" (0=No, 1=Yes).  \n",
    "Since URL is very long, we replace it with `'URL'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SAc_T2nla4m0"
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = str(w).lower().strip()\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    # replace urls\n",
    "    re_url = re.compile(r\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\\n",
    "                    .([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\",\n",
    "                    re.MULTILINE|re.UNICODE)\n",
    "    w = re_url.sub(\"URL\", w)\n",
    "\n",
    "    # # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lTVPxDyKbB6D"
   },
   "outputs": [],
   "source": [
    "def preprocess_label(l):\n",
    "    if l == 'Yes':\n",
    "         return 1\n",
    "    else:\n",
    "         return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OnEotbBBbha_"
   },
   "source": [
    "Let's preprocess our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LwEkjaLObJj1"
   },
   "outputs": [],
   "source": [
    "text_a = [preprocess_sentence(sent) for sent in df['SNIPPET_3192antibody'].values]\n",
    "text_b = [preprocess_sentence(sent) for sent in df['RRID Snippet_rdw'].values]\n",
    "labels = [preprocess_label(l) for l in df['RRID GT'].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vFdCvGepbrIF"
   },
   "source": [
    "## 3. Tokenization & Input Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NjJIC7aUb1QG"
   },
   "source": [
    "### 3.1 BERT Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7hngbUQbb6gH"
   },
   "source": [
    "To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n",
    "\n",
    "The tokenization must be performed by the tokenizer included with BERT--the below cell will download this for us. We'll be using the \"uncased\" version here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "G_drgcmYbll4",
    "outputId": "717ded8d-5d59-44f6-de9a-2e1f2e4e1f80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "InXhK-DMcPMP"
   },
   "source": [
    "### 3.2 Sentences to IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Uc814vacV21"
   },
   "source": [
    "The `tokenizer.encode` function combines multiple steps for us:\n",
    "1. Split the sentence into tokens.\n",
    "2. Add the special `[CLS]` and `[SEP]` tokens.\n",
    "3. Map the tokens to their IDs.\n",
    "\n",
    "Oddly, this function can perform truncating for us, but doesn't handle padding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "8Nag_3BHcE6c",
    "outputId": "cf366f34-5a03-49fb-be2a-9d97c41e441e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8130\n"
     ]
    }
   ],
   "source": [
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "input_ids = []\n",
    "for i in range(len(text_a)):\n",
    "    t = ['[CLS]']\n",
    "    token = tokenizer.tokenize(text_a[i])[:127]\n",
    "    t.extend(token)\n",
    "    t.append('[SEP]')\n",
    "    token = tokenizer.tokenize(text_b[i])[:126]\n",
    "    t.extend(token)\n",
    "    t.append('[SEP]')\n",
    "    input_ids.append(t)\n",
    "print(len(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "dhlNULKvcbGT",
    "outputId": "704a019f-4392-4470-d0e2-4b09a4cb7973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', 'the', 'di', '##athe', '##va', 'and', 'dahl', 'antibodies', 'were', 'two', 'of', 'the', 'five', 'the', 'authors', 'claim', 'to', 'be', 'non', 'specific', '.', 'it', 'has', 'always', 'been', 'the', 'case', 'that', 'specificity', 'of', 'antibodies', ',', 'particularly', 'when', 'used', 'on', 'intact', 'tissues', ',', 'is', 'hard', 'if', 'not', 'impossible', 'to', 'prove', '.', 'by', 'these', 'criteria', ',', 'the', 'results', 'obtained', 'in', 'western', 'blots', 'in', 'the', 'present', 'study', 'should', 'cause', 'us', 'to', 'discard', 'our', 'findings', '.', '[SEP]', 'the', 'four', 'pan', '##x', 'antibodies', 'used', 'in', 'this', 'study', 'were', 'a', 'commercial', 'chicken', 'anti', 'pan', '##x', 'di', '##athe', '##va', 'ant', ',', 'denoted', 'as', 'ckd', '##ia', ',', 'di', '##athe', '##va', ',', 'fan', '##o', ',', 'italy', 'nif', 'antibody', 'registry', 'number', 'ab', ',', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenize the first sentence:\")\n",
    "print (input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YxFVKwX_chHA"
   },
   "outputs": [],
   "source": [
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in input_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zyvVLbZ-cih_"
   },
   "source": [
    "### 3.3 Padding & Truncating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g9BNMpBrcqq3"
   },
   "source": [
    "Pad and truncate our sequences so that they all have the same length, `MAX_LEN`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gMC4y_hDcty7"
   },
   "source": [
    "First, what's the maximum sentence length in our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "HCL_0H85ciJ9",
    "outputId": "184eb64e-fb85-431d-f9df-6c2ad01d2d4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  256\n"
     ]
    }
   ],
   "source": [
    "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GUJaMQLnc0rH"
   },
   "source": [
    "Given that, let's choose MAX_LEN = 256 and apply the padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "colab_type": "code",
    "id": "u5Z_3AdTcxGL",
    "outputId": "c836aab5-e275-40ae-afb4-d315996a0748"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 256 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# We'll borrow the `pad_sequences` utility function to do this.\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set the maximum sequence length.\n",
    "# I've chosen 256 somewhat arbitrarily.\n",
    "MAX_LEN = 256\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "# Pad our input tokens with value 0.\n",
    "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
    "# as opposed to the beginning.\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print('\\nDone.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L2BBpQ2aeOxQ"
   },
   "source": [
    "### 3.4 Attention Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kBVBqmV_eTRF"
   },
   "source": [
    "The attention mask simply makes it explicit which tokens are actual words versus which are padding. \n",
    "\n",
    "The BERT vocabulary does not use the ID 0, so if a token ID is 0, then it's padding, and otherwise it's a real token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24JREmpReJVw"
   },
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# For each sentence...\n",
    "for sent in input_ids:\n",
    "    \n",
    "    # Create the attention mask.\n",
    "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    \n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SqqQut6oeqgi"
   },
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AdON67ZIiiL5"
   },
   "source": [
    "Define data spliting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H4QDXIuueMnM"
   },
   "outputs": [],
   "source": [
    "# Use KFold to split our data into train and validation sets for\n",
    "# training\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# define KFold with n_splits=5 to make the dataset split test_size=1/n_splits\n",
    "kf = KFold(n_splits=5, random_state=2018, shuffle=True)\n",
    "\n",
    "def kfold_split(x, y, round_loop=1):\n",
    "    count = 1\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        x_train, x_test, y_train, y_test = [x[i] for i in train_index], [x[i] for i in test_index], [y[i] for i in train_index], [y[i] for i in test_index]\n",
    "        if round_loop == count:\n",
    "            break\n",
    "        count += 1\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6FbawJThip0i"
   },
   "source": [
    "Define helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "50vF5F1sf-kA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JcGc353fgKOB"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9pznhpgRisVr"
   },
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FaObt1DGbIU3",
    "outputId": "a818b753-8594-4116-fb9d-b38d5a1183d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Starting fold:  0\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    407.    Elapsed: 0:00:33.\n",
      "  Batch    80  of    407.    Elapsed: 0:01:06.\n",
      "  Batch   120  of    407.    Elapsed: 0:01:39.\n",
      "  Batch   160  of    407.    Elapsed: 0:02:13.\n",
      "  Batch   200  of    407.    Elapsed: 0:02:46.\n",
      "  Batch   240  of    407.    Elapsed: 0:03:20.\n",
      "  Batch   280  of    407.    Elapsed: 0:03:53.\n",
      "  Batch   320  of    407.    Elapsed: 0:04:26.\n",
      "  Batch   360  of    407.    Elapsed: 0:05:00.\n",
      "  Batch   400  of    407.    Elapsed: 0:05:33.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epcoh took: 0:05:39\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  Validation took: 0:00:28\n",
      "Saving model to ./model_save/\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97      1391\n",
      "           1       0.76      0.89      0.82       235\n",
      "\n",
      "    accuracy                           0.94      1626\n",
      "   macro avg       0.87      0.92      0.89      1626\n",
      "weighted avg       0.95      0.94      0.95      1626\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    407.    Elapsed: 0:00:33.\n",
      "  Batch    80  of    407.    Elapsed: 0:01:07.\n",
      "  Batch   120  of    407.    Elapsed: 0:01:40.\n",
      "  Batch   160  of    407.    Elapsed: 0:02:13.\n",
      "  Batch   200  of    407.    Elapsed: 0:02:47.\n",
      "  Batch   240  of    407.    Elapsed: 0:03:20.\n",
      "  Batch   280  of    407.    Elapsed: 0:03:53.\n",
      "  Batch   320  of    407.    Elapsed: 0:04:27.\n",
      "  Batch   360  of    407.    Elapsed: 0:05:00.\n",
      "  Batch   400  of    407.    Elapsed: 0:05:33.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epcoh took: 0:05:39\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  Validation took: 0:00:28\n",
      "Saving model to ./model_save/\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      1391\n",
      "           1       0.92      0.83      0.87       235\n",
      "\n",
      "    accuracy                           0.96      1626\n",
      "   macro avg       0.94      0.91      0.92      1626\n",
      "weighted avg       0.96      0.96      0.96      1626\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    407.    Elapsed: 0:00:33.\n",
      "  Batch    80  of    407.    Elapsed: 0:01:07.\n",
      "  Batch   120  of    407.    Elapsed: 0:01:40.\n",
      "  Batch   160  of    407.    Elapsed: 0:02:13.\n",
      "  Batch   200  of    407.    Elapsed: 0:02:46.\n",
      "  Batch   240  of    407.    Elapsed: 0:03:20.\n",
      "  Batch   280  of    407.    Elapsed: 0:03:53.\n",
      "  Batch   320  of    407.    Elapsed: 0:04:26.\n",
      "  Batch   360  of    407.    Elapsed: 0:04:59.\n",
      "  Batch   400  of    407.    Elapsed: 0:05:33.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epcoh took: 0:05:38\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "  Validation took: 0:00:28\n",
      "Saving model to ./model_save/\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1391\n",
      "           1       0.91      0.86      0.88       235\n",
      "\n",
      "    accuracy                           0.97      1626\n",
      "   macro avg       0.94      0.92      0.93      1626\n",
      "weighted avg       0.97      0.97      0.97      1626\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    407.    Elapsed: 0:00:33.\n",
      "  Batch    80  of    407.    Elapsed: 0:01:07.\n",
      "  Batch   120  of    407.    Elapsed: 0:01:40.\n",
      "  Batch   160  of    407.    Elapsed: 0:02:13.\n",
      "  Batch   200  of    407.    Elapsed: 0:02:46.\n",
      "  Batch   240  of    407.    Elapsed: 0:03:20.\n",
      "  Batch   280  of    407.    Elapsed: 0:03:53.\n",
      "  Batch   320  of    407.    Elapsed: 0:04:26.\n",
      "  Batch   360  of    407.    Elapsed: 0:04:59.\n",
      "  Batch   400  of    407.    Elapsed: 0:05:33.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epcoh took: 0:05:38\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "  Validation took: 0:00:28\n",
      "Saving model to ./model_save/\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1391\n",
      "           1       0.89      0.91      0.90       235\n",
      "\n",
      "    accuracy                           0.97      1626\n",
      "   macro avg       0.94      0.94      0.94      1626\n",
      "weighted avg       0.97      0.97      0.97      1626\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Done!!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here.\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
    "# 16 or 32.\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "seed_val = 2018\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "max_acc = 0\n",
    "\n",
    "for fid in range(5):\n",
    "    print('>> Starting fold: ', fid)\n",
    "    print('>>'*50)\n",
    "    train_inputs, validation_inputs, train_labels, validation_labels = kfold_split(input_ids, labels, fid)\n",
    "    train_masks, validation_masks, _, _ = kfold_split(attention_masks, input_ids, fid)\n",
    "\n",
    "    # Convert all inputs and labels into torch tensors, the required datatype \n",
    "    # for our model.\n",
    "    train_inputs = torch.tensor(train_inputs)\n",
    "    validation_inputs = torch.tensor(validation_inputs)\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    validation_labels = torch.tensor(validation_labels)\n",
    "    train_masks = torch.tensor(train_masks)\n",
    "    validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "    # Create the DataLoader for our training set.\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create the DataLoader for our validation set.\n",
    "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "    validation_sampler = SequentialSampler(validation_data)\n",
    "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "    \n",
    "    # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "    # linear classification layer on top. \n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'allenai/scibert_scivocab_uncased', # Use the 12-layer BERT model, with an uncased vocab.\n",
    "        num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                        # You can increase this for multi-class tasks.   \n",
    "        output_attentions = False, # Whether the model returns attentions weights.\n",
    "        output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "    )\n",
    "\n",
    "    # Tell pytorch to run this model on the GPU.\n",
    "    model.cuda()\n",
    "\n",
    "    # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "    # I believe the 'W' stands for 'Weight Decay fix\"\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                    )\n",
    "\n",
    "    # Total number of training steps is number of batches * number of epochs.\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    # Create the learning rate scheduler.\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "    \n",
    "    # Store the average loss after each epoch so we can plot them.\n",
    "    loss_values = []\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        \n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_loss = 0\n",
    "\n",
    "        # Put the model into training mode. Don't be mislead--the call to \n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every 40 batches.\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                \n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "            # `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            # Always clear any previously calculated gradients before performing a\n",
    "            # backward pass. PyTorch doesn't do this automatically because \n",
    "            # accumulating the gradients is \"convenient while training RNNs\". \n",
    "            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "            model.zero_grad()        \n",
    "\n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # This will return the loss (rather than the model output) because we\n",
    "            # have provided the `labels`.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "            \n",
    "            # The call to `model` always returns a tuple, so we need to pull the \n",
    "            # loss value out of the tuple.\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # Accumulate the training loss over all of the batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and take a step using the computed gradient.\n",
    "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "            # modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over the training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)            \n",
    "        \n",
    "        # Store the loss value for plotting the learning curve.\n",
    "        loss_values.append(avg_train_loss)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "            \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            \n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            \n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            \n",
    "            # Telling the model not to compute or store gradients, saving memory and\n",
    "            # speeding up validation\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # This will return the logits rather than the loss because we have\n",
    "                # not provided labels.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this `model` function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                outputs = model(b_input_ids, \n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=b_input_mask)\n",
    "            \n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            y_pred.extend(np.argmax(logits, axis=1).flatten())\n",
    "            y_true.extend(label_ids.flatten())\n",
    "            \n",
    "            # Calculate the accuracy for this batch of test sentences.\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "            \n",
    "            # Accumulate the total accuracy.\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "            # Track the number of batches\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "        print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "        if max_acc < eval_accuracy/nb_eval_steps:\n",
    "            max_acc = eval_accuracy/nb_eval_steps\n",
    "            \n",
    "            output_dir = './model_save/'\n",
    "\n",
    "            # Create output directory if needed\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "\n",
    "            print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "            # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "            # They can then be reloaded using `from_pretrained()`\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "            model_to_save.save_pretrained(output_dir)\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "            # Good practice: save your training arguments together with the trained model\n",
    "            # torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
    "\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        print(\"  F1 score: {}\".format(f1_score(y_true, y_pred)))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "print(\"Done!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117
    },
    "colab_type": "code",
    "id": "R3damqxnTRqx",
    "outputId": "c3b94cfe-d4d9-4f17-9c21-ced3bfcfd04f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 429660K\n",
      "-rw-r--r-- 1 root root      1K Jun 26 07:03 config.json\n",
      "-rw-r--r-- 1 root root 429423K Jun 26 07:03 pytorch_model.bin\n",
      "-rw-r--r-- 1 root root      1K Jun 26 07:03 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root      1K Jun 26 07:03 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root    223K Jun 26 07:03 vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -l --block-size=K ./model_save/\n",
    "!cp -r ./model_save/ '/content/gdrive/My Drive/antibody-watch/antibody-rrid/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P7YzsveESra6"
   },
   "source": [
    "## For evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DrfQYd9jTT3D",
    "outputId": "1fc1d624-7de6-4090-ada2-1abf95104898"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = './model_save/'\n",
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = BertForSequenceClassification.from_pretrained(output_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Copy the model to the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "7Lhr9SuqStTK",
    "outputId": "fca28076-f5cc-4b75-b1d4-ff408da9a524"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>PMID</th>\n",
       "      <th>RRID Snippet_rdw_x</th>\n",
       "      <th>SNIPPET_3192antibody_x</th>\n",
       "      <th>RRID GT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>23390418</td>\n",
       "      <td>The four Panx1 antibodies used in this study w...</td>\n",
       "      <td>The Diatheva and Dahl antibodies were two of t...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>23390418</td>\n",
       "      <td>(2) chicken anti-Panx1 (obtained from Gerhard ...</td>\n",
       "      <td>The Diatheva and Dahl antibodies were two of t...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>23390418</td>\n",
       "      <td>(3) rabbit anti-Panx1 (denoted as Rb57; NIF an...</td>\n",
       "      <td>The Diatheva and Dahl antibodies were two of t...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>23390418</td>\n",
       "      <td>and (4) mouse anti-Panx1 (denoted as Mo503; NI...</td>\n",
       "      <td>The Diatheva and Dahl antibodies were two of t...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>23390418</td>\n",
       "      <td>The four Panx1 antibodies used in this study w...</td>\n",
       "      <td>For example, in the study of Panx1 knockout mi...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      PMID  ...                             SNIPPET_3192antibody_x RRID GT\n",
       "0   0  23390418  ...  The Diatheva and Dahl antibodies were two of t...     Yes\n",
       "1   1  23390418  ...  The Diatheva and Dahl antibodies were two of t...     Yes\n",
       "2   2  23390418  ...  The Diatheva and Dahl antibodies were two of t...      No\n",
       "3   3  23390418  ...  The Diatheva and Dahl antibodies were two of t...      No\n",
       "4   4  23390418  ...  For example, in the study of Panx1 knockout mi...     Yes\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataPath+'all-pair.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "QPGlhHV3S1zX",
    "outputId": "cc8cc196-837f-40ae-c078-f31590e2e738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 25,130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "text_a = [preprocess_sentence(sent) for sent in df['SNIPPET_3192antibody_x'].values]\n",
    "text_b = [preprocess_sentence(sent) for sent in df['RRID Snippet_rdw_x'].values]\n",
    "# labels = [preprocess_label(l) for l in df['Label: Yes/No'].values]\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for i in range(len(text_a)):\n",
    "  t = ['[CLS]']\n",
    "  token = tokenizer.tokenize(text_a[i])[:127]\n",
    "  t.extend(token)\n",
    "  t.append('[SEP]')\n",
    "  token = tokenizer.tokenize(text_b[i])[:127]\n",
    "  t.extend(token)\n",
    "  input_ids.append(t)\n",
    "\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in input_ids]\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask) \n",
    "\n",
    "# Convert to tensors.\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "eRntXC8wS3NY",
    "outputId": "e6a1f574-908e-462e-f3a6-242c2a601897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 25,130 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "# predictions , true_labels = [], []\n",
    "predictions = []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  # b_input_ids, b_input_mask, b_labels = batch\n",
    "  b_input_ids, b_input_mask = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  # label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(np.argmax(logits, axis=1).flatten())\n",
    "  # true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eugz87HOS7Qe"
   },
   "source": [
    "### Save to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aa7W55z6S6Ip"
   },
   "outputs": [],
   "source": [
    "def pred_to_label(pred):\n",
    "  if pred == 1:\n",
    "    return 'Yes'\n",
    "  return 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QlNNqfE4S94p"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def PredictGenerator(data, prediction, filename='prediction.csv'):\n",
    "    num_record = len(list(data.id.values))\n",
    "    count = 0\n",
    "    labels = []\n",
    "    for i in prediction:\n",
    "      for j in i:\n",
    "        labels.append(j)\n",
    "    with open(filename, mode='w') as csv_file:\n",
    "      fieldnames = ['id', 'PMID','ground truth', 'prediction', 'SNIPPET_3192antibody', 'Snippet_rdw']\n",
    "      writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "      writer.writeheader()\n",
    "      for label in labels:\n",
    "        if count < num_record:\n",
    "          writer.writerow({\n",
    "            'id': data.id.values[count],\n",
    "            'PMID': data.PMID.values[count], \n",
    "            'ground truth': data['RRID GT'].values[count], \n",
    "            'prediction': pred_to_label(label), \n",
    "            'SNIPPET_3192antibody': data['SNIPPET_3192antibody_x'].values[count],\n",
    "            'Snippet_rdw': data['RRID Snippet_rdw_x'].values[count]\n",
    "          })\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FZA49hR9VEM0"
   },
   "outputs": [],
   "source": [
    "PredictGenerator(df, predictions, folderPath+'evaluation-rrid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sWc3UAOmfpnw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SciBERT-new.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}